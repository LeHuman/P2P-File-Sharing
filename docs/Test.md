---
geometry:

- margin=1in
- letterpaper

classoption:    table
documentclass:  extarticle
urlcolor:       blue
fontsize:       11pt
header-includes: |
    \usepackage{multicol}
    \usepackage{graphicx}
    \usepackage{float}
...

\normalsize
\pagenumbering{gobble}
\pagenumbering{arabic}

\graphicspath{ {img/} }

**CS 550 - Spring 2022**  
**Isaias Rivera**  
**A20442116**

# P2P File Sharing - Consistency - Tests

A special binary was used which does not include the interactive terminal and helps automate some of the testing.
This *test peer* does the following.

- Files are randomly generated where it is ensured each peer has their own unique files.
- Active peers will make requests for files and randomly delete local files.
- After requesting a file, active test peers will record the percentage of invalid files that the request found before it was returned onto a csv file.
- Non active peers will simply randomly make modifications to their own files, invalidating files

Due to the overwhelming number of requests to the filesystem, I had to put a delay between sequential calls to request/delete files. Otherwise, requests seemed to hang and timeout. However, calls between peers, when they happen, are still concurrent. In addition to that, one issue with the design is that the directory the program is watching will auto index files even if they are in the middle of downloading, this means files that take too long to download will actually be indexed in this half downloaded state. There are checks at the time of peer to peer connection to ensure peers actually have the file and this is only really an issue with larger files, but ultimately these requests will fail as the file no longer exists by the time a request is made. Regardless, the indexing server still processes the requests by peers and peers should be ready when this happens.

In every test, each peer aims to run 30 calls, only the actual requesting of a file from super-peers counts towards this. For each client, the response time for each request of this type is stored to a csv.
The binaries used are compiled as release, meaning it should run as efficient as they can, however, this also means any issues I ran into were not easily debuggable.

Communication between peer to peer and peer to super-peer indexing server have timeouts, meaning, if there is sufficient traffic it is possible that requests might fail. I did not get around to implementing some method to externally catch and log these errors.

Furthermode, due to time constraints, the TTR value that is used when in pulling mode was very small compared to what the PA suggested. I tested using TTRs of 5s, 15s, and 30s.

\newpage

## Local Tests

These tests were run locally on the same windows machine, for both peers and super-peers.

### 10 Super-peers and 19 Leaf-nodes

These tests were mostly automated using a python script which read the csv files generated by the test peers and then compiled it into averages.
I then took these averages and graphed them in Excel.

This test concurrently ran 29 test peers where 10 of them were super peers. Only N number of these peers are actively making requests, where N ranged from 1 to 5.

This test is performed both in pushing and pulling mode and both the linear and all to all topology, where pulling mode tests also varied the TTR by 5s, 15s, and 30s.

#### Topology setup

\mbox{}

This is a graphic which visualizes how each client is setup using the static configuration.

\includegraphics[width=\textwidth]{TestConfig.png}

\newpage

#### Results

\mbox{}

This graph plots both the times for the linear and all to all topologies.

\includegraphics[width=\textwidth]{PA2Graph.png}

BROKEN : ADD INFO ABOUT PUSH VS PULL

#### Issues

